{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Αντίγραφο του lmtcBERT.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["ehDEyECPgGLY","APPwlwoZ5I4v","g2flVEp81vFV"],"toc_visible":true,"authorship_tag":"ABX9TyNOPq9OMFtVI9hl63kal4Gw"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MiMQ7Oa3-7WB"},"source":["---\n","\n","\n","**Large MultiLabel Text Classification**\n","> AM DS1190015\n","> <br>Apostolos Papatheodorou \n","\n","---"]},{"cell_type":"code","metadata":{"id":"pn1cTLyXqTXt"},"source":["import time\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from os import listdir\n","import time; import json\n","from datetime import date\n","from os.path import isfile, join"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOYO8kpHZz1W"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XC-aPb4qjfVK"},"source":["# Path to Folder with pickles files (pickles for Raptarchis)\n","\n","%cd /content/drive/My Drive/dissertation/RAPTARCHIS47k \t\n","!ls\n","print()\n","%cd /content/drive/My Drive/dissertation/thesis \t\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-L_bZJiOwkjK"},"source":["\n","---\n","# ` Preprocessing Section `  \n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5VjisvJOTdd3"},"source":["### **Data Loading**"]},{"cell_type":"code","metadata":{"id":"YvQ5J1gQ4MoH"},"source":["# Load Training Set from pickles \n","\n","with open('pickles/Tr_OnlyFiles.pkl', 'rb') as f:\n","  tr_onlyfiles=pickle.load(f)\n","\n","with open('pickles/Tr_Year.pkl', 'rb') as f:\n","  tr_year=pickle.load(f)\n","\n","with open('pickles/Tr_Header.pkl', 'rb') as f:\n","  tr_headers=pickle.load(f)\n","\n","with open('pickles/Tr_Volume.pkl', 'rb') as f:\n","  tr_vols=pickle.load(f)\n","\n","with open('pickles/Tr_Chapter.pkl', 'rb') as f:\n","  tr_chaptrs=pickle.load(f)\n","\n","with open('pickles/Tr_Subject.pkl', 'rb') as f:\n","  tr_subjs=pickle.load(f)\n","\n","with open('pickles/Tr_Articles.pkl', 'rb') as f:\n","  tr_artcls=pickle.load(f)\n","\n","\n","# Load Development Set from pickles \n","\n","with open('pickles/Dev_OnlyFiles.pkl', 'rb') as f:\n","  dev_onlyfiles=pickle.load(f)\n","\n","with open('pickles/Dev_Year.pkl', 'rb') as f:\n","  dev_year=pickle.load(f)\n","\n","with open('pickles/Dev_Volume.pkl', 'rb') as f:\n","  dev_vols=pickle.load(f)\n","\n","with open('pickles/Dev_Chapter.pkl', 'rb') as f:\n","  dev_chaptrs=pickle.load(f)\n","\n","with open('pickles/Dev_Subject.pkl', 'rb') as f:\n","  dev_subjs=pickle.load(f)\n","\n","with open('pickles/Dev_Header.pkl', 'rb') as f:\n","  dev_headers=pickle.load(f)\n","\n","with open('pickles/Dev_Articles.pkl', 'rb') as f:\n","  dev_artcls=pickle.load(f)\n","\n","\n","# Load Test Set from pickles \n","\n","with open('pickles/Test_OnlyFiles.pkl', 'rb') as f:\n","  test_onlyfiles=pickle.load(f)\n","\n","with open('pickles/Test_Year.pkl', 'rb') as f:\n","  test_year=pickle.load(f)\n","\n","with open('pickles/Test_Volume.pkl', 'rb') as f:\n","  test_vols=pickle.load(f)\n","\n","with open('pickles/Test_Chapter.pkl', 'rb') as f:\n","  test_chaptrs=pickle.load(f)\n","\n","with open('pickles/Test_Subject.pkl', 'rb') as f:\n","  test_subjs=pickle.load(f)\n","\n","with open('pickles/Test_Header.pkl', 'rb') as f:\n","  test_headers=pickle.load(f)\n","\n","with open('pickles/Test_Articles.pkl', 'rb') as f:\n","  test_artcls=pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQzSf0iR27a9"},"source":["import re\n","import os\n","import csv\n","import nltk\n","import string\n","import pandas as pd\n","import seaborn as sns\n","nltk.download('stopwords')\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","from sklearn import preprocessing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yw4S7GUqMYRV"},"source":["REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","Gr_STOPWORDS = set(stopwords.words('greek'))\n","puncList = [\".\",\";\",\":\",\"!\",\"?\",\"/\",\",\",\"#\",\"@\",\"$\",\"&\",\")\",\"(\",\"\\\"\", \">\", \"<\", \"=\", \"{\", \"}\", \"|\", \"-\"]\n","punc=\"\".join(puncList)  #\"\\\\\"\n","t = str.maketrans(dict.fromkeys(punc, \" \"))\n","\n","print(string.punctuation)\n","print(punc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ku8Diqvqqwbk"},"source":["---\n","**Input Data:**\n","\n","* **Text = [ Header + Articles ]**\n","\n","* **text = [ Header + Articles ] - Stopwords**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"ud3E8-Pb_lnj"},"source":["# Clear headers and articles columns of Raptarchis DataSet\n","# Text list removes greeek stopwords but text list keeps them\n","\n","def Clean_Text(heads, arts):\n","\n","  Text=[]; text=[]\n","  for i in range(len(heads)):\n","    item=heads[i].replace('\\n', ' ')+' '+arts[i].replace('\\n', ' ')\n","    item=\" \".join(item.split() )\n","    item=item.lower()\n","    item=\" \".join(item.translate(t).split())\n","    \n","    Text.append(item)\n","  \n","    tokens = [t for t in item.split(' ') if t not in Gr_STOPWORDS]\n","    item = \" \".join(tokens)\n","  \n","    text.append( item.translate(str.maketrans('', '', punc)) )\n","  return Text, text\n","\n","tr_Text, tr_text = Clean_Text(tr_headers, tr_artcls)\n","dev_Text, dev_text = Clean_Text(dev_headers, dev_artcls)\n","test_Text, test_text = Clean_Text(test_headers, test_artcls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Vi-RC6z08GW"},"source":["print(\"Dev Set:\", len(dev_Text), len(dev_text))\n","print(\"Train Set:\", len(tr_Text),  len(tr_text))\n","print(\"Test Set:\", len(test_Text),  len(test_text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhw32A2IDfVb"},"source":["# Create DataFrame with the Data of Raptarchis DataSet\n","# Dataframe Columns:[Header, Year, Text/text, Categories: [Volume, Chpater, Subject] ]\n","\n","\n","# Training Set: \n","df = pd.DataFrame({'OnlyFiles':tr_onlyfiles}) # Define the DataFrame \n","\n","df[\"text\"]  = tr_text;    df[\"Text\"] =    tr_Text  # [Text, text]\n","df[\"Volume\"]= tr_vols;    df[\"Chapter\"] = tr_chaptrs; df[\"Subject\"]= tr_subjs\n","df[\"Header\"]= tr_headers; df[\"Articles\"]= tr_artcls;  df[\"Year\"]   = tr_year\n","\n","\n","# Testing Set:\n","df_test = pd.DataFrame({'OnlyFiles':test_onlyfiles})\n","\n","df_test[\"text\"]  = test_text;    df_test[\"Text\"]    = test_Text   # [Text, text]\n","df_test[\"Volume\"]= test_vols;    df_test[\"Chapter\"] = test_chaptrs; df_test[\"Subject\"]= test_subjs \n","df_test[\"Header\"]= test_headers; df_test[\"Articles\"]= test_artcls;  df_test[\"Year\"]   = test_year\n","\n","# Validation Set:\n","df_dev = pd.DataFrame({'OnlyFiles':dev_onlyfiles})\n","\n","df_dev[\"text\"]  = dev_text;    df_dev[\"Text\"] =    dev_Text  # [Text, text]\n","df_dev[\"Volume\"]= dev_vols;    df_dev[\"Chapter\"] = dev_chaptrs; df_dev[\"Subject\"]= dev_subjs \n","df_dev[\"Header\"]= dev_headers; df_dev[\"Articles\"]= dev_artcls;  df_dev[\"Year\"]   = dev_year"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aubzf4rnIUGK"},"source":["print(df.count()); print(); print(df_dev.count() ); print(); print(df_test.count() )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GGLK4VXtBqs"},"source":["# Keep only Frequent Labels.\n","# A label is Frequent when has Apprearences > Freq_Num\n","Freq_Num = 50 \n","\n","vol_filter = df.groupby('Volume').filter(lambda x: len(x) >= Freq_Num)\n","ch_filter  = vol_filter.groupby('Chapter').filter(lambda x: len(x) >= Freq_Num)\n","sub_filter = ch_filter.groupby('Subject').filter(lambda x: len(x) >= Freq_Num )\n","dffreq= sub_filter\n","\n","# Few-shot Datasets: [ Chpaters & Subjects ]\n","few_ch=df.groupby('Chapter').filter(lambda x: len(x) < Freq_Num)\n","few_sub=df.groupby('Subject').filter(lambda x: len(x) < Freq_Num)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CJWnPLF2l9Aa"},"source":["# Count the number of data \n","\n","print(dffreq.count());print() # Frequent Dataset (volumes, chapters subjects)\n","print(few_ch.count());print() # Few-shot Dataset (chapters)\n","print(few_sub.count())        # Few-shot Dataset (subjects)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lw4bBz8T72ai"},"source":["# Print Unique Classes for Freq-shot Dataset(Vols, Chs, Subs)\n","print(\"Freq-shot dataset:  \", dffreq[\"Volume\"].unique().shape, dffreq[\"Chapter\"].unique().shape,\n","      dffreq[\"Subject\"].unique().shape)\n","\n","# Print Unique Classes for Few-shot Datasets\n","print(\"Few-shot datasets:  \", few_ch[\"Chapter\"].unique().shape, few_sub[\"Subject\"].unique().shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_Naa70t6JBi"},"source":["# Dev Set: Keep Values that Already exist in Training Set\n","\n","df_dev = df_dev[ df_dev['Volume'].isin(  list( dffreq[\"Volume\"].unique()) )]\n","df_dev = df_dev[ df_dev['Chapter'].isin( list( dffreq[\"Chapter\"].unique()) )]\n","df_dev = df_dev[ df_dev['Subject'].isin( list( dffreq[\"Subject\"].unique()) )]\n","\n","\n","# Test Set: Keep Values that Already exist in Training Set\n","\n","df_test = df_test[ df_test['Volume'].isin(  list( dffreq[\"Volume\"].unique()) )]\n","df_test = df_test[ df_test['Chapter'].isin( list( dffreq[\"Chapter\"].unique()) )]\n","df_test = df_test[ df_test['Subject'].isin( list( dffreq[\"Subject\"].unique()) )]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_iDw5Uvi506e"},"source":["# Validation Set\n","print(\"\\t\\t==================  VALIDATION SET  ==================\\n\")\n","\n","# Print Volume Categories in Train and Dev Set\n","p1=list( dffreq[\"Volume\"].unique() ); p1.sort(); \n","p2=list( df_dev[\"Volume\"].unique() ); p2.sort();\n","print( len(p1), p1 ); print(  len(p2), p2 ); print()\n","\n","# Print Chapter Categories in Train and Dev Set\n","p1=list( dffreq[\"Chapter\"].unique() ); p1.sort(); \n","p2=list( df_dev[\"Chapter\"].unique() ); p2.sort()\n","print( len(p1), p1 ); print(  len(p2), p2 ); print()\n","\n","# Print Subject Categories in Train and Dev Set\n","p1=list( dffreq[\"Subject\"].unique() ); p1.sort(); \n","p2=list( df_dev[\"Subject\"].unique() ); p2.sort()\n","print( len(p1), p1 ); print(  len(p2), p2 ); print(\"\\n\\n\")\n","\n","\n","# Testing Set\n","print(\"\\t\\t==================  TESTING SET  ==================\\n\") \n","\n","# Print Volume Categories in Train and Test Set\n","p1=list( dffreq[\"Volume\"].unique() ); p1.sort(); \n","p2=list( df_test[\"Volume\"].unique() ); p2.sort();\n","print( len(p1), p1 ); print(  len(p2), p2 ); print()\n","\n","# Print Chapter Categories in Train and Test Set\n","p1=list( dffreq[\"Chapter\"].unique() ); p1.sort(); \n","p2=list( df_test[\"Chapter\"].unique() ); p2.sort()\n","print( len(p1), p1 ); print(  len(p2), p2 ); print()\n","\n","# Print Subject Categories in Train and Test Set\n","p1=list( dffreq[\"Subject\"].unique() ); p1.sort(); \n","p2=list( df_test[\"Subject\"].unique() ); p2.sort()\n","print( len(p1), p1 ); print(  len(p2), p2 );"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hz7YcaB3IdDa"},"source":["# For Frequent Dataset\n","\n","# Count the number of Classes for each hierarchical level\n","Unique_Vol= len(dffreq.Volume.unique())  # Level-1: Volumes\n","Unique_Ch = len(dffreq.Chapter.unique()) # Level-2: Chapters\n","Unique_Sub= len(dffreq.Subject.unique()) # Level-3: Subjects\n","\n","# Convert Lables to Numerical form\n","vol_label_dict = {}; ch_label_dict = {}; sub_label_dict = {}\n","\n","\n","# Convert Volumes-Names to Numbers \n","possible_labels = dffreq.Volume.unique()\n","for index, possible_label in enumerate(possible_labels):\n","    vol_label_dict[possible_label] = index\n","\n","# Convert Chapters-Names to Numbers \n","possible_labels = dffreq.Chapter.unique()\n","for index, possible_label in enumerate(possible_labels):\n","    ch_label_dict[possible_label] = index\n","\n","# Convert Subjects-Names to Numbers \n","possible_labels = dffreq.Subject.unique()\n","for index, possible_label in enumerate(possible_labels):\n","    sub_label_dict[possible_label] = index\n","\n","\n","# Save New Columns to: [Train, Dev, Test] Set\n","\n","# dffreq dataframe \n","dffreq['Vol_label'] = dffreq.Volume.replace(vol_label_dict)\n","dffreq['Ch_label' ] = dffreq.Chapter.replace(ch_label_dict)\n","dffreq['Sub_label'] = dffreq.Subject.replace(sub_label_dict)\n","\n","# df_dev dataframe \n","df_dev['Vol_label'] = df_dev.Volume.replace(vol_label_dict)\n","df_dev['Ch_label' ] = df_dev.Chapter.replace(ch_label_dict)\n","df_dev['Sub_label'] = df_dev.Subject.replace(sub_label_dict)\n","\n","# df_test dataframe \n","df_test['Vol_label'] = df_test.Volume.replace(vol_label_dict)\n","df_test['Ch_label' ] = df_test.Chapter.replace(ch_label_dict)\n","df_test['Sub_label'] = df_test.Subject.replace(sub_label_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vTafUOFIdHB"},"source":["# Print Volumes Chapters and Subjects Numbers \n","print(Unique_Vol,  vol_label_dict.items())\n","print(Unique_Ch,  ch_label_dict.items())\n","print(Unique_Sub, sub_label_dict.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gV2h8MuBFNT8"},"source":["# Total Number of Test/ Train/ Dev. Instances\n","print( f\"Total Testing Set: {   df_test.OnlyFiles.count()}\")\n","print( f\"Total Training Set: {  dffreq.OnlyFiles.count() }\")\n","print( f\"Total Developemnt Set: {df_dev.OnlyFiles.count()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXESX02_Uzn2"},"source":["# Returns the key of a given Dict Value\n","def get_key(Dict, val):\n","    l = []\n","    for key, value in Dict.items():\n","         if val == value:\n","             l.append( key )\n","    return l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oh7WKUIjg94k"},"source":["# Optionally:\n","\n","# Save Dataframes to pickles\n","dffreq.to_pickle(\"pickles/dataframes/TrainDF.pkl\")\n","df_dev.to_pickle(\"pickles/dataframes/ValidDF.pkl\")\n","df_test.to_pickle(\"pickles/dataframes/TestDF.pkl\")\n","\n","# Store data (serialize)\n","with open(\"pickles/dataframes/VolDict.pkl\", 'wb') as handle:\n","    pickle.dump(vol_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# Store data (serialize)\n","with open(\"pickles/dataframes/ChDict.pkl\", 'wb') as handle:\n","    pickle.dump(ch_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# Store data (serialize)\n","with open(\"pickles/dataframes/SubDict.pkl\", 'wb') as handle:\n","    pickle.dump(sub_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHseBWQAD7MU"},"source":["---\n","**Test DataFrames**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"U6W6NPST-2Ih"},"source":["# Checkc Dev Dataframe\n","df_test.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulXCQl5iYdTA"},"source":["# Checkc Dev Dataframe\n","df_dev.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iTIUdKfJ-dm"},"source":["# Check Train Dataframe\n","dffreq.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lev5hV-JvsSu"},"source":["---\n","### **Matrices**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"dkbdS0E8l6LC"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBy2hodpiXQA"},"source":["# Optionally\n","\n","# Load data from pickles\n","dffreq_pckl = pd.read_pickle(\"pickles/dataframes/TrainDF.pkl\")\n","df_dev_pckl = pd.read_pickle(\"pickles/dataframes/ValidDF.pkl\")\n","df_test_pckl = pd.read_pickle(\"pickles/dataframes/TestDF.pkl\")\n","\n","# load data (serialize)\n","with open(\"pickles/dataframes/VolDict.pkl\", 'rb') as handle:\n","    vol_label_dict_pckl = pickle.load(handle)\n","\n","# Store data (serialize)\n","with open(\"pickles/dataframes/ChDict.pkl\", 'rb') as handle:\n","    ch_label_dict_pckl = pickle.load(handle)\n","\n","# Store data (serialize)\n","with open(\"pickles/dataframes/SubDict.pkl\", 'rb') as handle:\n","    sub_label_dict_pckl = pickle.load(handle)\n","\n","\n","# Count the number of Classes for each hierarchical level\n","Unique_Vol= len(dffreq.Volume.unique())  # Level-1: Volumes\n","Unique_Ch = len(dffreq.Chapter.unique()) # Level-2: Chapters\n","Unique_Sub= len(dffreq.Subject.unique()) # Level-3: Subjects"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBf7hYzlkCU4"},"source":["print(dffreq_pckl.equals(dffreq), df_dev_pckl.equals(df_dev), df_test_pckl.equals(df_test), \"\\n\")\n","\n","print(vol_label_dict)\n","print(vol_label_dict_pckl)\n","print((vol_label_dict_pckl==vol_label_dict), \"\\n\" )\n","\n","print(ch_label_dict)\n","print(ch_label_dict_pckl)\n","print((ch_label_dict_pckl==ch_label_dict), \"\\n\" )\n","\n","print(sub_label_dict)\n","print(sub_label_dict_pckl)\n","print((sub_label_dict_pckl==sub_label_dict) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2y6FWoY3lcIt"},"source":["dffreq_pckl.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoFN-o7xkBBA"},"source":["dffreq.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ph63kbOX9zU"},"source":["---\n","**Ch_matrix: The matrix which masks the Chapter predictions.**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"z1SC6XyJzZba"},"source":["# dffreq Columns\n","print(list(dffreq.columns), '\\n'); \n","\n","# Print Volumes Chapters and Subjects Numbers \n","print(Unique_Vol, vol_label_dict.items())\n","print(Unique_Ch,  ch_label_dict.items())\n","print(Unique_Sub, sub_label_dict.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XygKx0YPwEF2"},"source":["# Ch_matrix: The Matrix which masks the Chapter predictions\n","\n","# Create np.matrix(( |Volumes|*|Chapters| ))\n","# Find Unique Volumes \n","Vols = dffreq.Volume.unique() \n","\n","Vol_2_Ch = {} # Dict (Len = Vol_len) for the chapters of each volume.\n","Ch_matrix = np.zeros((Unique_Vol, Unique_Ch)) # Matrix: ((|Volumes|*|Chapters|))\n","\n","\n","for vol in Vols:\n","  \n","  l = [];\n","  i = vol_label_dict[vol]\n","  UniqChapters = dffreq.loc[dffreq['Volume'] == vol].Chapter.unique()\n","  print( vol_label_dict.get(vol), vol, UniqChapters )\n","  \n","  for ch in UniqChapters:\n","    j = ch_label_dict.get(ch)\n","    l.append( j )\n","    Ch_matrix[i, j] = 1.0\n","\n","  Vol_2_Ch[i] = l\n","  print(Vol_2_Ch[i], np.nonzero(Ch_matrix[i]))\n","  print(Ch_matrix[i,:]); print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sEEs_Ccuz8_K"},"source":["# Some subjects may be children of more than one Chapter nodes: \n","# eg. [\"ΔΙΑΦΟΡΕΣ ΔΙΑΤΑΞΕΙΣ\", 'ΓΕΝΙΚΕΣ ΔΙΑΤΑΞΕΙΣ' ] =ids [11, 100]\n","\n","print(get_key(sub_label_dict, 100))\n","\n","# Subject cat has Chapter parents:\n","cat = \"ΔΙΑΦΟΡΕΣ ΔΙΑΤΑΞΕΙΣ\"\n","c= dffreq[dffreq[\"Subject\"].isin([ cat ]) ]\n","L =c['Chapter'].unique()\n","print(len(L), end= ': ')\n","for l in L:\n","  print( ch_label_dict[l], l, end=', ' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8802Lc80zho1"},"source":["print(Vol_2_Ch)\n","print(get)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-tu5vStkhtT"},"source":["---\n","**Test Chapter_Matrix**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"T82iHAyS-y4N"},"source":["# Chapter-Matrix Analysis: \n","print(\"Chapter Matrix:\", Ch_matrix.shape); print()\n","\n","print(\"Sum(Ch_matrix, Columns) len:\", len(Ch_matrix.sum(0)),); print(Ch_matrix.sum(0) ); print()\n","\n","print(\"Sum(Ch_matrix, Rows) len:\", len(Ch_matrix.sum(1)),   ); print(Ch_matrix.sum(1) ) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"th5kXjz-wEOd"},"source":["# Test the usage of Ch_matrix\n","# Suppose that the Vol-predictions are everywhere zeros\n","# Except for indx1 and idx2 positions.\n","\n","pred = np.zeros(Unique_Vol)\n","\n","idx1 = 13; pred[idx1] = 22\n","idx2 = 20; pred[idx2] = 44\n","\n","# The dot prodcut of (pred and Ch_matrix) = length of Chapters\n","npred = np.dot(pred, Ch_matrix)\n","\n","# Which is zero everwhere apart from the childrens of idx1 and idx2\n","print(len(Vol_2_Ch[idx1]), len(Vol_2_Ch[idx2]), np.count_nonzero(npred))\n","print(Vol_2_Ch[idx1], Vol_2_Ch[idx2], np.nonzero(npred))\n","print(npred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-ZMzgjbdnPv"},"source":["# Check the process with pytorch Tensors of Batch_size = 4\n","\n","# Tensor has dimensions: Batch_size * VOL_Size * CH_Size\n","s = torch.ones(4, Unique_Vol, Unique_Ch )\n","s[:,] = torch.from_numpy(Ch_matrix)\n","\n","# The models output is zeros, except some random positions.\n","a = torch.zeros(4, Unique_Vol)\n","a[0,0] = 10; a[1, 2] = 20; a[1, 4] =21; a[2,-1] =100; a[3,5] = 200\n","a= a.unsqueeze(1)\n","print(a.shape, a)\n","\n","# Calculate the batch dot product\n","ot= torch.bmm(a, s).squeeze(1)\n","print(ot )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4lUQmlCjTmv"},"source":["# Now do the mask of predicted Chapter values\n","# Supose to be that predictions on Chapter are everywhere ones\n","\n","# Mask unwanted values\n","a = torch.ones(4, Unique_Ch)*2\n","print(a.shape, ot.shape, )\n","\n","# Make the tensor Masking\n","ot2 = torch.mul(a, ot)\n","print(ot2.shape, ot2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7A-M8OucL2X"},"source":["---\n","**Sub_matrix: The matrix which masks the Subject predictions.**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"JmHuab6OPjmC"},"source":["print(sub_label_dict[\"ΓΕΝΙΚΕΣ ΔΙΑΤΑΞΕΙΣ\"])\n","print(ch_label_dict); print(sub_label_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lmb9mEy-PkDM"},"source":["# Sub_matrix: The matrix which masks the Subject predictions\n","\n","# Create np.matrix(( |Chapters|*|Subjects| ))\n","# Find Unique Chapters \n","UniqChapters = dffreq.Chapter.unique() \n","\n","Ch_2_Sub = {} # Dict (Len=Vol_len) for the Subjects of each Chapter.\n","Sub_matrix = np.zeros((Unique_Ch, Unique_Sub)) # Matrix:((|Volumes|*|Chapters|))\n","\n","for ch in UniqChapters:\n","  \n","  l = [];\n","  i = ch_label_dict[ch]\n","  UniqSubjects = dffreq.loc[dffreq['Chapter'] == ch].Subject.unique()\n","  print( ch_label_dict.get(ch), ch,  UniqSubjects )\n","  \n","  for sub in UniqSubjects:\n","    j = sub_label_dict.get(sub)\n","    l.append( j )\n","    Sub_matrix[i, j] = 1\n","\n","  Ch_2_Sub[i] = l\n","  print(Ch_2_Sub[i], np.nonzero(Sub_matrix[i]))\n","  print(Sub_matrix[i,:], \"\\n\");  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUmANuZumjuS"},"source":["---\n","**Test Subject Matrix**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"RYXBEhoEsQH0"},"source":["# Test the usage of Sub_matrix on Batch Matrix Multiplication\n","# Createe a Tensor (Batch_size, Chapter_Len, Subjects_Len)\n","s = torch.ones(4, Unique_Ch, Unique_Sub )\n","s[:,] = torch.from_numpy(Sub_matrix)\n","\n","# The models output\n","a = torch.zeros(4, Unique_Ch)\n","a[0,0] = 10; a[1, 102] = 20; a[1, 79] =21; a[2,-1] =100; a[3,2] = 200\n","a= a.unsqueeze(1)\n","\n","ot= torch.bmm(a, s).squeeze(1)\n","\n","print(a.shape, s.shape)\n","print(ot )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_LJDAfhsQH8"},"source":["# Mask unwanted values example\n","a = torch.ones(4, Unique_Sub)*2\n","print(a.shape, ot.shape, )\n","ot2 = torch.mul(a, ot)\n","print(ot2.shape, ot2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8iaccpAo6mP"},"source":["---\n","**Test Subject Matrix**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"9BgnUjdrOQrW"},"source":["# print Sub_matrix Analysis:\n","print(\"Subject Matrix:\", Sub_matrix.shape); print()\n","print( \"Sum(Sub_matrix, Columns) len:\", len(Sub_matrix.sum(0)))\n","print( Sub_matrix.sum(0), ) ; print()\n","print( \"Sum(Sub_matrix, Rows) len:\",len(Sub_matrix.sum(1)))\n","print( Sub_matrix.sum(1), ) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGeteSwpRTCE"},"source":["# Some subjects may be children of more than one Chapter nodes: \n","# eg. [\"ΔΙΑΦΟΡΕΣ ΔΙΑΤΑΞΕΙΣ\", 'ΓΕΝΙΚΕΣ ΔΙΑΤΑΞΕΙΣ' ] =ids [11, 100]\n","\n","print(get_key(sub_label_dict, 100))\n","\n","# Subject cat has Chapter parents:\n","cat = \"ΓΕΝΙΚΕΣ ΔΙΑΤΑΞΕΙΣ\"\n","c= dffreq[dffreq[\"Subject\"].isin([ cat ]) ]\n","L =c['Chapter'].unique()\n","print(len(L), end= ': ')\n","for l in L:\n","  print( ch_label_dict[l], l, end=', ' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7NVnh9I0PkDO"},"source":["# Test the usage of Sub_matrix\n","# Suppose that the Chapter-predictions are everywhere zeros\n","# Except for indx1 and idx2 positions.\n","pred = np.zeros(Unique_Ch)\n","\n","idx1 = 5; pred[idx1] = 11\n","idx2 = 6; pred[idx2] = 100\n","\n","# The dot prodcut of (pred and Ch_matrix) = length of Chapters\n","npred = np.dot(pred, Sub_matrix)\n","\n","# Which is zero everwhere apart from the childrens of idx1 and idx2pred = np.zeros(Unique_Ch)\n","print(npred)\n","print(len(Ch_2_Sub[idx1]), len(Ch_2_Sub[idx2]), np.count_nonzero(npred))\n","print(Ch_2_Sub[idx1], Ch_2_Sub[idx2], np.nonzero(npred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"na_RgBCMvw1q"},"source":["# Save Ch_matrix and Sub_matrix as pickles\n","with open('pickles/dataframes/Ch.pkl','wb') as f:\n","    pickle.dump(Ch_matrix, f)\n","\n","with open('pickles/dataframes/Sub.pkl','wb') as f:\n","    pickle.dump(Sub_matrix, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMTmRT_yYeM8"},"source":["---\n","**END OF PREPROCESSING SECTION**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"4g7LRMbAeGci"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4M1hqNQTom3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehDEyECPgGLY"},"source":["### **DataFrame**"]},{"cell_type":"code","metadata":{"id":"UqJNYwvYx53g"},"source":["import matplotlib.pyplot as plt; plt.rcdefaults()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","objects = mlb.classes_\n","y_pos = np.arange(len(objects))\n","performance = datay.sum(axis=0)\n","\n","plt.bar(y_pos, performance, align='center', alpha=0.5)\n","plt.xticks(y_pos, objects)\n","plt.ylabel('Samples')\n","plt.title('Category')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zypwWL67s8en"},"source":["print(df.loc[22].Text)\n","print(df.loc[22].text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KFktjifItFDb"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTPXsTLuf-tF"},"source":["print(df['Volume'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNPs8Sd1uj6S"},"source":["few_sub.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a684G0_8uO_u"},"source":["few_ch.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sf95xCSO1HvS"},"source":["print(few_sub['Chapter'].count())\n","print(few_ch['Chapter'].value_counts())\n","#print(few_sub['Subject'].count())\n","#print(few_sub['Subject'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHeP6OeM1FqG"},"source":["print(vol_filter['Volume'].value_counts())\n","print(ch_filter['Chapter'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"shtslz_S0oPk"},"source":["print(df['Chapter'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5Kt7OO-tkPf"},"source":["print(ch_filter.count()); print('\\n')\n","\n","print(ch_filter['Chapter'].value_counts());  print('\\n')\n","\n","ch_filter.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VzlXTyVm0wJw"},"source":["print(df['Subject'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpMsikjnxRJn"},"source":["print(sub_filter.count()); print('\\n')\n","\n","print(sub_filter['Subject'].value_counts());  print('\\n')\n","\n","sub_filter.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APPwlwoZ5I4v"},"source":["---\n","\n","**Data Analysis**\n","---\n","---"]},{"cell_type":"code","metadata":{"id":"dGxp3pdSeo9J"},"source":["df.Header.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTa1WcbHf7nH"},"source":["count_summary = df['Volume'].value_counts()\n","\n","plt.figure(figsize=(8,4))\n","sns.barplot(count_summary.index, count_summary.values, alpha=0.8)\n","plt.ylabel('Number of Occurrences', fontsize=12)\n","plt.xlabel('Volume Label', fontsize=12)\n","plt.show()\n","\n","count_summary = df['Chapter'].value_counts()\n","\n","plt.figure(figsize=(8,4))\n","sns.barplot(count_summary.index, count_summary.values, alpha=0.8)\n","plt.ylabel('Number of Occurrences', fontsize=12)\n","plt.xlabel('Chapter Label', fontsize=12)\n","plt.show()\n","\n","count_summary = df['Subject'].value_counts()\n","\n","plt.figure(figsize=(8,4))\n","sns.barplot(count_summary.index, count_summary.values, alpha=0.8)\n","plt.ylabel('Number of Occurrences', fontsize=12)\n","plt.xlabel('Subject Class', fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_hZWD4FgPaf"},"source":["count_summary = df['Volume'].value_counts()\n","\n","plt.figure(figsize=(8,4))\n","sns.barplot(count_summary.index, count_summary.values, alpha=0.8)\n","plt.ylabel('Number of Occurrences', fontsize=12)\n","plt.xlabel('Author Name', fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xf_h6BS535mh"},"source":["**Group By Label**"]},{"cell_type":"code","metadata":{"id":"5j01vbNsectG"},"source":["from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","wc = WordCloud(background_color=\"white\", max_words=5000, \n","               stopwords=STOPWORDS, max_font_size= 50)\n","# generate word cloud\n","wc.generate(\" \".join(df.Text.values))\n","\n","# show\n","plt.imshow(wc, interpolation='bilinear')\n","plt.title(\"Words from ALL Author\", fontsize=20,color='seagreen')\n","plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33UpeRyUfaE-"},"source":["wc = WordCloud(background_color=\"white\", max_words=5000, \n","               stopwords=Gr_STOPWORDS, max_font_size= 50)\n","# generate word cloud\n","wc.generate(\" \".join(df.Text.values))\n","\n","# show\n","plt.imshow(wc, interpolation='bilinear')\n","plt.title(\"Words from ALL Author\", fontsize=20,color='seagreen')\n","plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5DzeN_i0slj"},"source":["grouped_tags = df.groupby(\"Volume\", sort='count').size().reset_index(name='count')\n","fig = plt.figure(figsize=(8,6))\n","grouped_tags.plot(figsize=(8,4), title=\"Volume frequency\")\n","\n","grouped_tags = df.groupby(\"Chapter\", sort='count').size().reset_index(name='count')\n","fig = plt.figure(figsize=(8,6))\n","grouped_tags.plot(figsize=(8,4), title=\"Chapter frequency\")\n","\n","grouped_tags = df.groupby(\"Subject\", sort='count').size().reset_index(name='count')\n","fig = plt.figure(figsize=(8,6))\n","grouped_tags.plot(figsize=(8,4), title=\"Subject frequency\")\n","\n","grouped_tags = df.groupby(\"Year\", sort='count').size().reset_index(name='count')\n","fig = plt.figure(figsize=(8,6))\n","grouped_tags.plot(figsize=(8,4), title=\"Year frequency\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNEl5M_q0stf"},"source":["counts = df.Volume.value_counts()\n","firstlast = counts[:5].append(counts[-5:])\n","firstlast.reset_index(name=\"count\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IFUIsGVy40JO"},"source":["counts = df.Chapter.value_counts()\n","firstlast = counts[:5].append(counts[-5:])\n","firstlast.reset_index(name=\"count\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSzjIwVy42X9"},"source":["counts = df.Subject.value_counts()\n","firstlast = counts[:5].append(counts[-5:])\n","firstlast.reset_index(name=\"count\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1mnc-S8MYT2"},"source":["print('Length of Greek Stopwords:', len(Gr_STOPWORDS))\n","print('Length of French Stopwords:', len(Fr_STOPWORDS))\n","print('Len. of English Stopwords:', len(En_STOPWORDS))\n","\n","print()\n","print(Gr_STOPWORDS)\n","print(Fr_STOPWORDS)\n","print(En_STOPWORDS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78-BljZx0cc_"},"source":["---\n","\n","# **`Training SECTION`**\n","---"]},{"cell_type":"markdown","metadata":{"id":"V4sArlBd1j5M"},"source":["**Install Transformers**\n"]},{"cell_type":"code","metadata":{"id":"T_aaG5iHkIM8"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujG3wkaDIhLV"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTqXVry-HlCD"},"source":["import torch\n","import itertools\n","from transformers import *\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"heTmYJ2hGfJj"},"source":["# Greek-BERT\n","tokenizer = AutoTokenizer.from_pretrained('nlpaueb/bert-base-greek-uncased-v1')\n","model = AutoModelWithLMHead.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', output_hidden_states=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5vgZrTzPfYC"},"source":["# MultiLingual-BERT\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n","model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\",  output_hidden_states = True,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6yNBSEqwb2L"},"source":["text = \"Μια ακόμα ελληνική πρόταση\"\n","encoded_input = tokenizer(text, return_tensors='pt')\n","print(encoded_input.keys(), '\\n', encoded_input)\n","print(encoded_input[\"input_ids\"][0].tolist()  )\n","print(tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0].tolist()))\n","\n","print()\n","output = model(**encoded_input)\n","\n","print(len(output), output.keys())\n","print(len(output[0][0]), output[0])\n","print(len(output[1][0]), output[1][0][0:10])\n","print()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GfmrTFaeeU4B"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"auJhfYUezC8e"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJzqcz96iUxt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CX-sIDrk1A8A"},"source":["---\n","**Start Training**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"6Ccuw7DH8Jxi"},"source":["# Use gpu is available\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","print(device, device ==\"cuda\" )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Djlyk-wj5DUk"},"source":["# Parameters Configuration\n","\n","# Two available Models [\"Greek\" and \"Multilingual\"]\n","\n","BERT_MODEL=\"Greek\"\n","#BERT_MODEL=\"Multilingual\"\n","\n","# Defining training variables for training\n","MAX_LEN = 400\n","MAX_PATIENCE = 3\n","TRAIN_BATCH_SIZE = 4\n","VALID_BATCH_SIZE = 4\n","TEST_BATCH_SIZE  = 4\n","EPOCHS = 5\n","LEARNING_RATE = 1e-05\n","\n","\n","# Define Tokenizers\n","if (BERT_MODEL=='Greek'):\n","  print(\"GREEK-BERT\")\n","  tokenizer = AutoTokenizer.from_pretrained('nlpaueb/bert-base-greek-uncased-v1')\n","\n","elif (BERT_MODEL==\"Multilingual\" ):\n","  print(\"MultiLingual-BERT\")\n","  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2vX7kzaAHu39"},"source":["# Define a Class for Input Tokenisation\n","# Functions: Truncate sentences to max_len, padding them, convert tokens to ids\n","\n","class Triage():\n","  def __init__(self, dataframe, tokenizer, max_len):\n","      self.len = len(dataframe)\n","      self.data = dataframe\n","      self.tokenizer = tokenizer\n","      self.max_len = max_len\n","        \n","  def __getitem__(self, index):\n","        \n","      title = str(self.data.Text[index])\n","      title = \" \".join(title.split())\n","      inputs = self.tokenizer.encode_plus(\n","          title, None, \n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          pad_to_max_length=True,\n","          return_token_type_ids=True,\n","          truncation=True\n","      )\n","      ids = inputs['input_ids']\n","      mask = inputs['attention_mask']\n","\n","      #print(self.data.OnlyFiles[index], self.data.Vol_label[index], self.data.Ch_label[index], self.data.Sub_label[index], self.data.Text[index])\n","\n","      return {\n","          'ids': torch.tensor(ids, dtype=torch.long),\n","          'mask': torch.tensor(mask, dtype=torch.long),\n","          'target_vol': torch.tensor(self.data.Vol_label[index], dtype=torch.long),\n","          'target_ch': torch.tensor(self.data.Ch_label[index], dtype=torch.long),\n","          'target_sub': torch.tensor(self.data.Sub_label[index], dtype=torch.long),\n","      } \n","    \n","  def __len__(self):\n","      return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-si2GMPQNhy"},"source":["# Create the dataset and dataloader for the Model\n","\n","train_dataset= dffreq.reset_index(drop=True)\n","dev_dataset  = df_dev.reset_index(drop=True)\n","test_dataset = df_test.reset_index(drop=True)\n","\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"VALID Dataset: {}\".format(dev_dataset.shape))\n","print(\"TEST Dataset:  {}\".format(test_dataset.shape))\n","\n","training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n","develop_set  = Triage(dev_dataset, tokenizer,  MAX_LEN)\n","testing_set  = Triage(test_dataset, tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iY43ky-jXiLu"},"source":["# convert ids to string\n","#tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(training_set.__getitem__(3)[\"ids\"]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"COm3G2yBQNko"},"source":["print(training_set.__getitem__(3)[\"ids\"])\n","print()\n","print(testing_set.__getitem__(3)[\"mask\"])\n","print()\n","print(develop_set.__getitem__(3)[\"mask\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMBv-jMxMGQk"},"source":["# Passs Data to DataLoader \n","\n","train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","dev_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': TEST_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","develop_loader  = DataLoader(develop_set, **dev_params)\n","testing_loader  = DataLoader(testing_set, **test_params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBeQVGz8IRfG"},"source":["### Models"]},{"cell_type":"code","metadata":{"id":"kDg5_ZKoMGWI"},"source":["# Define SimpleBERT-Class for training \n","\n","class SimpleBERT(torch.nn.Module):\n","  def __init__(self, bert_model, vol_num, ch_num, sub_num):\n","      super(SimpleBERT, self).__init__()\n","\n","      self.vol_num = vol_num  # Number of Volumes  nodes\n","      self.ch_num = ch_num    # Numver of Chapters nodes\n","      self.sub_num = sub_num  # Number of Subjects nodes\n","      self.bert_model=bert_model # String varialble \n","\n","      # bert_model: Greek or  Multilingual Edition of BERT\n","      if (self.bert_model==\"Greek\"):\n","        print('GREEK-BERT')\n","        self.l1 = AutoModelWithLMHead.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', output_hidden_states=True)\n","      \n","      elif (self.bert_model=='Multilingual') :\n","        print('MultiLingual-BERT')\n","        self.l1 = BertModel.from_pretrained(\"bert-base-multilingual-uncased\",  output_hidden_states = True)\n","\n","\n","      # Use DNNs after bert (not used now)\n","      # self.pre_classifier = torch.nn.Linear(768, 768)\n","      # self.dropout = torch.nn.Dropout(0.3)\n","      \n","      # Final Outputs\n","      self.Vol_Classifier = torch.nn.Linear(768, self.vol_num) # Volume Classifier\n","      self.Ch_Classifier = torch.nn.Linear(768, self.ch_num)   # Chapter Classifier\n","      self.Sub_Classifier = torch.nn.Linear(768, self.sub_num) # Subject Classifier\n","\n","      # Chapter Classifier\n","      # Subject Classifier\n","      \n","      \n","  def forward(self, input_ids, attention_mask):\n","      output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","      \n","      \n","      # Keep Last Layer [CLS] output\n","      pooler=output_1[\"hidden_states\"][ -1][:, 0] \n","\n","      \n","      # No use of DNNs -- Optionaly --\n","      #pooler = self.pre_classifier(pooler)\n","      #pooler = torch.nn.ReLU()(pooler)\n","      #pooler = self.dropout(pooler)\n","\n","      # Final Results [Volume, Chapter, Subject]\n","      vol_output = self.Vol_Classifier(pooler) # Volume Output\n","      ch_output = self.Ch_Classifier(pooler)   # Chapter Output\n","      sub_output = self.Sub_Classifier(pooler) # Chapter Output\n","\n","      return vol_output, ch_output, sub_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6JiR6bzWVcG0"},"source":["# Define Last3_BERT-Class for training \n","\n","class Last3_BERT(torch.nn.Module):\n","  def __init__(self, bert_model, vol_num, ch_num, sub_num):\n","      super(Last3_BERT, self).__init__()\n","\n","      self.vol_num = vol_num  # Number of Volumes  nodes\n","      self.ch_num = ch_num    # Numver of Chapters nodes\n","      self.sub_num = sub_num  # Number of Subjects nodes\n","      self.bert_model=bert_model # String varialble \n","\n","      # bert_model: Greek or  Multilingual Edition of BERT\n","      if (self.bert_model==\"Greek\"):\n","        print('GREEK-BERT')\n","        self.l1 = AutoModelWithLMHead.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', output_hidden_states=True)\n","      \n","      elif (self.bert_model=='Multilingual') :\n","        print('MultiLingual-BERT')\n","        self.l1 = BertModel.from_pretrained(\"bert-base-multilingual-uncased\",  output_hidden_states = True)\n","\n","\n","      # Use DNNs after bert (not used now)\n","      # self.pre_classifier = torch.nn.Linear(768, 768)\n","      # self.dropout = torch.nn.Dropout(0.3)\n","      \n","      # Final Outputs\n","      self.Vol_Classifier = torch.nn.Linear(768, self.vol_num) # Volume Classifier\n","      self.Ch_Classifier = torch.nn.Linear(768, self.ch_num)   # Chapter Classifier\n","      self.Sub_Classifier = torch.nn.Linear(768, self.sub_num) # Subject Classifier\n","\n","      # Chapter Classifier\n","      # Subject Classifier\n","      \n","      \n","  def forward(self, input_ids, attention_mask):\n","      output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","      \n","      \n","      # Keep Last Layer [CLS] output\n","      pooler_1 = output_1[\"hidden_states\"][ -1][:, 0] \n","      pooler_2 = output_1[\"hidden_states\"][ -2][:, 0]\n","      pooler_3 = output_1[\"hidden_states\"][ -3][:, 0]\n","\n","      \n","      # No use of DNNs -- Optionaly --\n","      #pooler = self.pre_classifier(pooler)\n","      #pooler = torch.nn.ReLU()(pooler)\n","      #pooler = self.dropout(pooler)\n","\n","      # Final Results [Volume, Chapter, Subject]\n","      vol_output = self.Vol_Classifier(pooler_3)  # Volume  Output\n","      ch_output  = self.Ch_Classifier( pooler_2)  # Chapter Output\n","      sub_output = self.Sub_Classifier(pooler_1)  # Subject Output\n","\n","      return vol_output, ch_output, sub_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IFVittfb0pT"},"source":["# Define Mask_SimpleBERT-Class for training \n","\n","class Mask_SimpleBERT(torch.nn.Module):\n","  def __init__(self, bert_model, vol_num, ch_num, sub_num, Chapter_matrix, Subject_matrix, Batch_size = 4 ):\n","      super(Mask_SimpleBERT, self).__init__()\n","\n","      self.vol_num = vol_num  # Number of Volumes  nodes\n","      self.ch_num = ch_num    # Numver of Chapters nodes\n","      self.sub_num = sub_num  # Number of Subjects nodes\n","      self.bert_model=bert_model # String varialble \n","\n","      self.Batch_size = Batch_size  # Batch size    \n","      self.Chapter_matrix = Chapter_matrix # Matrix_Len = |Volumes|*|Chapters|\n","      self.Subject_matrix = Subject_matrix # Matrix_Len = |Chapters|*|Subjects|\n","\n","      # Tensors For Masking\n","      if (device==\"cuda\"):\n","        print(\"GPU Available\")\n","        self.Chapter_Tensor = torch.ones(self.Batch_size, self.Chapter_matrix.shape[0], self.Chapter_matrix.shape[1] ).to(device)\n","        self.Subject_Tensor = torch.ones(self.Batch_size, self.Subject_matrix.shape[0], self.Subject_matrix.shape[1] ).to(device) \n","      else:\n","        print(\"No GPU Available\")\n","        self.Chapter_Tensor = torch.ones(self.Batch_size, self.Chapter_matrix.shape[0], self.Chapter_matrix.shape[1] )\n","        self.Subject_Tensor = torch.ones(self.Batch_size, self.Subject_matrix.shape[0], self.Subject_matrix.shape[1] ) \n","    \n","      self.Chapter_Tensor[:, ] = torch.from_numpy(self.Chapter_matrix)\n","      self.Subject_Tensor[:, ] = torch.from_numpy(self.Subject_matrix)\n","\n","\n","      # bert_model: Greek or  Multilingual Edition of BERT\n","      if (self.bert_model==\"Greek\"):\n","        print('GREEK-BERT')\n","        self.l1 = AutoModelWithLMHead.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', output_hidden_states=True)\n","      \n","      elif (self.bert_model=='Multilingual') :\n","        print('MultiLingual-BERT')\n","        self.l1 = BertModel.from_pretrained(\"bert-base-multilingual-uncased\",  output_hidden_states = True)\n","\n","      \n","      # Final Outputs\n","      self.Vol_Classifier = torch.nn.Linear(768, self.vol_num) # Volume Classifier\n","      self.Ch_Classifier = torch.nn.Linear(768, self.ch_num)   # Chapter Classifier\n","      self.Sub_Classifier = torch.nn.Linear(768, self.sub_num) # Subject Classifier\n","\n","\n","      \n","  def forward(self, input_ids, attention_mask):\n","      output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","      \n","      # Keep Last Layer [CLS] output\n","      pooler=output_1[\"hidden_states\"][ -1][:, 0] \n","\n","      # Final Results [Volume, Chapter, Subject]\n","      vol_output = self.Vol_Classifier(pooler) # Volume Output\n","      ch_output = self.Ch_Classifier(pooler)   # Chapter Output\n","      sub_output = self.Sub_Classifier(pooler) # Chapter Output\n","\n","\n","      mch_out  = torch.bmm(vol_output.unsqueeze(1), self.Chapter_Tensor).squeeze(1)\n","      msub_out = torch.bmm(ch_output.unsqueeze(1), self.Subject_Tensor).squeeze(1)\n","\n","      # Convert Volume/Chapter outputs to probabilities\n","      #mch_out = torch.nn.functional.softmax(mch_out)\n","      #msub_out = torch.nn.functional.softmax(msub_out)\n","\n","      mch_output  = torch.mul(ch_output, mch_out)\n","      msub_output = torch.mul(sub_output, msub_out)\n","\n","      return vol_output, mch_output, msub_output\n","      #return vol_output, ch_output, sub_output , mch_output, msub_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVzKHdBjMYVt"},"source":["# Define SumLast_BERT-Class for training \n","\n","class SumLast_BERT(torch.nn.Module):\n","  def __init__(self, bert_model, vol_num, ch_num, sub_num):\n","      super(SumLast_BERT, self).__init__()\n","\n","      self.vol_num = vol_num  # Number of Volumes  nodes\n","      self.ch_num = ch_num    # Numver of Chapters nodes\n","      self.sub_num = sub_num  # Number of Subjects nodes\n","      self.bert_model=bert_model # String varialble \n","\n","      # bert_model: Greek or  Multilingual Edition of BERT\n","      if (self.bert_model==\"Greek\"):\n","        print('GREEK-BERT')\n","        self.l1 = AutoModelWithLMHead.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', output_hidden_states=True)\n","      \n","      elif (self.bert_model=='Multilingual') :\n","        print('MultiLingual-BERT')\n","        self.l1 = BertModel.from_pretrained(\"bert-base-multilingual-uncased\",  output_hidden_states = True)\n","\n","\n","      # Use DNNs after bert (not used now)\n","      # self.pre_classifier = torch.nn.Linear(768, 768)\n","      # self.dropout = torch.nn.Dropout(0.3)\n","      \n","      # Final Outputs\n","      self.Vol_Classifier = torch.nn.Linear(768, self.vol_num) # Volume Classifier\n","      self.Ch_Classifier = torch.nn.Linear(768, self.ch_num)   # Chapter Classifier\n","      self.Sub_Classifier = torch.nn.Linear(768, self.sub_num) # Subject Classifier\n","\n","      # Chapter Classifier\n","      # Subject Classifier\n","      \n","      \n","  def forward(self, input_ids, attention_mask):\n","      output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","      \n","      \n","      # Keep Last Layer [CLS] output\n","      \n","      pooler = output_1[\"hidden_states\"][-1][:,1:-1].sum(1 ) \n","      pooler = pooler/(MAX_LEN-2)\n","      #print(output_1[\"hidden_states\"][-1][:,1:-1].shape)\n","      #print(pooler)\n","      #print(pooler/(MAX_LEN-2))\n","      #print(pooler.shape)\n","\n","      \n","      #pooler=output_1[\"hidden_states\"][ -1][:, 0] \n","\n","      \n","      # No use of DNNs -- Optionaly --\n","      #pooler = self.pre_classifier(pooler)\n","      #pooler = torch.nn.ReLU()(pooler)\n","      #pooler = self.dropout(pooler)\n","\n","      # Final Results [Volume, Chapter, Subject]\n","      vol_output = self.Vol_Classifier(pooler) # Volume Output\n","      ch_output = self.Ch_Classifier(pooler)   # Chapter Output\n","      sub_output = self.Sub_Classifier(pooler) # Chapter Output\n","\n","      return vol_output, ch_output, sub_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyfVibYDiZtY"},"source":["# Define SumLast4_BERT-Class for training \n","\n","class SumLast4_BERT(torch.nn.Module):\n","  def __init__(self, bert_model, vol_num, ch_num, sub_num):\n","      super(SumLast4_BERT, self).__init__()\n","\n","      self.vol_num = vol_num  # Number of Volumes  nodes\n","      self.ch_num = ch_num    # Numver of Chapters nodes\n","      self.sub_num = sub_num  # Number of Subjects nodes\n","      self.bert_model=bert_model # String varialble \n","\n","      # bert_model: Greek or  Multilingual Edition of BERT\n","      if (self.bert_model==\"Greek\"):\n","        print('GREEK-BERT')\n","        self.l1 = AutoModelWithLMHead.from_pretrained('nlpaueb/bert-base-greek-uncased-v1', output_hidden_states=True)\n","      \n","      elif (self.bert_model=='Multilingual') :\n","        print('MultiLingual-BERT')\n","        self.l1 = BertModel.from_pretrained(\"bert-base-multilingual-uncased\",  output_hidden_states = True)\n","\n","\n","      # Use DNNs after bert (not used now)\n","      # self.pre_classifier = torch.nn.Linear(768, 768)\n","      # self.dropout = torch.nn.Dropout(0.3)\n","      \n","      # Final Outputs\n","      self.Vol_Classifier = torch.nn.Linear(768, self.vol_num) # Volume Classifier\n","      self.Ch_Classifier = torch.nn.Linear(768, self.ch_num)   # Chapter Classifier\n","      self.Sub_Classifier = torch.nn.Linear(768, self.sub_num) # Subject Classifier\n","\n","      # Chapter Classifier\n","      # Subject Classifier\n","      \n","      \n","  def forward(self, input_ids, attention_mask):\n","      output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","      \n","      \n","      # Keep Last Layer [CLS] output\n","      \n","      pooler_1 = output_1[\"hidden_states\"][-1][:,1:-1].sum(1 ) \n","      pooler_1 = pooler_1/(MAX_LEN-2)\n","\n","      pooler_2 = output_1[\"hidden_states\"][-2][:,1:-1].sum(1 ) \n","      pooler_2 = pooler_2/(MAX_LEN-2)\n","\n","      pooler_3 = output_1[\"hidden_states\"][-3][:,1:-1].sum(1 ) \n","      pooler_3 = pooler_3/(MAX_LEN-2)\n","\n","      pooler_4 = output_1[\"hidden_states\"][-4][:,1:-1].sum(1 ) \n","      pooler_4 = pooler_4/(MAX_LEN-2)\n","\n","      pooler = (pooler_1 + pooler_2 + pooler_2 + pooler_4)/4\n","\n","      # Final Results [Volume, Chapter, Subject]\n","      vol_output = self.Vol_Classifier(pooler) # Volume Output\n","      ch_output = self.Ch_Classifier(pooler)   # Chapter Output\n","      sub_output = self.Sub_Classifier(pooler) # Chapter Output\n","\n","      return vol_output, ch_output, sub_output "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVqWLOFYIf71"},"source":["### **Training**"]},{"cell_type":"code","metadata":{"id":"jy1cUvPBZUUU"},"source":["# Define an instance of the BERT model\n","\n","# All the Available Bert-Models\n","\n","#model = SimpleBERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub,)\n","#model = Last3_BERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub,)\n","#model = Mask_SimpleBERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub,\n","#                        Chapter_matrix= Ch_matrix, Subject_matrix=Sub_matrix)\n","model = SumLast4_BERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub,)\n","\n","\n","model.to(device)\n","\n","# Define the loss function and optimizer\n","loss_function = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJNQwbT4M7Db"},"source":["\"\"\"\n","for i ,data in enumerate(develop_loader, 0):\n","  ids = data['ids'].to(device, dtype = torch.long)\n","  mask = data['mask'].to(device, dtype = torch.long)\n","\n","  # Target values for Volume, Chapter and Subject Levels\n","  target_vol = data['target_vol'].to(device, dtype = torch.long)\n","  target_ch = data['target_ch'].to(device, dtype = torch.long)\n","  target_sub = data['target_sub'].to(device, dtype = torch.long)\n","\n","  vol, ch, sub  = model(ids, mask)  \n","\n","  \n","  print(Ch_matrix.shape, type(Ch_matrix))\n","  print(Sub_matrix.shape, type(Sub_matrix))\n","  print(vol[0].shape, type(target_sub))\n","  print()\n","\n","\n","  vol_loss = loss_function(vol, target_vol)#.item()\n","  ch_loss  = loss_function(ch , target_ch ).item()\n","  sub_loss = loss_function(sub, target_sub).item()\n","  print(vol_loss, ch_loss, sub_loss)\n","\n","  vol_big_val, vol_big_idx = torch.max(vol.data, dim=1)\n","  print(vol_big_idx, target_vol)\n","\n","\n","  ch_big_val, ch_big_idx  = torch.max(ch.data, dim=1)\n","  print(ch_big_idx, target_ch)\n","  #mch_big_val, mch_big_idx  = torch.max(mch.data, dim=1)\n","  #print(\"m\", mch_big_idx, target_ch, \"\\n\")\n","\n","  sub_big_val, sub_big_idx = torch.max(sub.data, dim=1)\n","  print(sub_big_idx, target_sub )\n","  #msub_big_val, msub_big_idx = torch.max(msub.data, dim=1)\n","  #print(\"m\", msub_big_idx, target_sub, \"\\n\" )\n","\n","\n","  vol_acc, inst1 = calculate_accu(vol_big_idx, target_vol) \n","  ch_acc,  inst2 = calculate_accu(ch_big_idx, target_ch)   \n","  sub_acc, inst3 = calculate_accu(sub_big_idx, target_sub) \n","\n","  insts= torch.tensor([ inst1.tolist(), inst2.tolist(), inst3.tolist()])\n","  print(insts)\n","\n","  p1=torch.ones(4)*3\n","  total_acc = (insts.sum(dim=0)==p1).sum().item()/4\n","  print(f\"Acc: {total_acc} [ {vol_acc}, {ch_acc}, {sub_acc}]\" )\n","\n","\n","  break\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gj9MiMHNX_RH"},"source":["# Function to calcuate the Class accuracy \n","def calculate_accu(big_idx, targets):\n","  \n","  n_correct = (big_idx==targets).sum().item()\n","  return n_correct/targets.shape[0]  , big_idx==targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IgO123hJZUXB"},"source":["# Training Function tuning the GreekBERT model\n","\n","def train(epoch, check_point=150, BATCH_SIZE=TRAIN_BATCH_SIZE ):\n","\n","    tr_loss=[];     tr_acc=[]  # Loss/Acc-list for each iteration\n","    tr_vol_acc=[];  tr_ch_acc=[];  tr_sub_acc=[]; # Save acc for each level \n","    tr_vol_loss=[]; tr_ch_loss=[]; tr_sub_loss=[];# Save Loss for each level \n","\n","    model.train()\n","    for i ,data in enumerate(training_loader, 0):    \n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask= data['mask'].to(device, dtype = torch.long)\n","\n","        # Target values for Volume, Chapter and Subject Levels\n","        target_vol = data['target_vol'].to(device, dtype = torch.long)\n","        target_ch  = data['target_ch'].to(device, dtype = torch.long )\n","        target_sub = data['target_sub'].to(device, dtype = torch.long)\n","        if (target_vol.shape[0]!=BATCH_SIZE): break\n","\n","\n","        vol, ch, sub = model(ids, mask) # Model prediction\n","        \n","\n","        # Loss estimation (sum losses of each category)\n","        vol_loss = loss_function(vol, target_vol)#.item()\n","        ch_loss  = loss_function(ch , target_ch )#.item()\n","        sub_loss = loss_function(sub, target_sub)#.item()\n","\n","        loss = vol_loss + ch_loss + sub_loss\n","        \n","\n","        # Calculate accuracy       \n","        vol_big_val, vol_big_idx = torch.max(vol.data, dim=1)\n","        ch_big_val, ch_big_idx  = torch.max(ch.data, dim=1)\n","        sub_big_val, sub_big_idx = torch.max(sub.data, dim=1)\n","      \n","        vol_acc, inst1 = calculate_accu(vol_big_idx, target_vol) # Acc. For Volumes\n","        ch_acc,  inst2 = calculate_accu(ch_big_idx, target_ch)   # Acc. For Chapters\n","        sub_acc, inst3 = calculate_accu(sub_big_idx, target_sub) # Acc. For Subjects\n","\n","        insts= torch.tensor([ inst1.tolist(), inst2.tolist(), inst3.tolist()])\n","\n","        total_acc = (insts.sum(dim=0)==p1).sum().item()/4  # Acc for [Volumes && Chapaers && Subjects]\n","              \n","\n","        # Save Loss and Accuracy for the Current Iteration\n","        tr_vol_loss.append(vol_loss.item()); tr_vol_acc.append(vol_acc);\n","        tr_ch_loss.append(ch_loss.item());   tr_ch_acc.append(ch_acc);\n","        tr_sub_loss.append(sub_loss.item()); tr_sub_acc.append(sub_acc);\n","        tr_loss.append(loss.item());         tr_acc.append(total_acc);\n","    \n","        # Upadate Weights\n","        optimizer.zero_grad(); loss.backward(); optimizer.step()\n","\n","        # Check point to Print outputs during the training process\n","        if ( (i % check_point == 0) and (i!=0) ):\n","          \n","          print(f\"\\n\\n\\t **************************    Train--Check--point {i}      **************************\")\n","          loss_step = sum(tr_loss[-check_point:]) / len( tr_loss[-check_point:] )\n","          \n","          loss_vol_step = sum(tr_vol_loss[-check_point:]) / len( tr_vol_loss[-check_point:] )\n","          loss_ch_step  = sum(tr_ch_loss[-check_point: ]) / len( tr_ch_loss[-check_point: ] )\n","          loss_sub_step = sum(tr_sub_loss[-check_point:]) / len( tr_sub_loss[-check_point:] )\n","          \n","          \n","          acc_step  = sum(tr_acc[-check_point:]) / len(tr_acc[-check_point:])\n","\n","          acc_vol_step = sum(tr_vol_acc[-check_point:]) / len(tr_vol_acc[-check_point:])\n","          acc_ch_step  = sum(tr_ch_acc[-check_point: ]) / len(tr_ch_acc[-check_point: ])\n","          acc_sub_step = sum(tr_sub_acc[-check_point:]) / len(tr_sub_acc[-check_point:])\n","\n","          print(f\"\\t Training Acc. per {check_point} last steps: {acc_step} [ {acc_vol_step}, {acc_ch_step}, {acc_sub_step}] \")\n","          print(f\"\\t Training Loss per {check_point} last steps: {loss_step} [ {loss_vol_step}, {loss_ch_step}, {loss_sub_step}] \")\n","          \n","          print(f\"\\n\\t Last {15} predictions:\")\n","          print('\\t', [np.round(ls,3) for ls in tr_loss[-10:] ])\n","          print('\\t', tr_acc[-15:] )\n","          print()\n","\n","          # Print --Optionally--\n","          print(\"\\n\\tLast Instance:\")\n","          print(\"\\t\\t\",vol_big_idx, target_vol);  print('\\t\\t', ch_big_idx, target_ch); \n","          print(\"\\t\\t\",sub_big_idx, target_sub);  print('\\n\\t\\t', insts)\n","          print()\n","          print(f\"\\t\\tAcc: {total_acc} [ {vol_acc}, {ch_acc}, {sub_acc}]\")\n","          print(f\"\\t\\tLoss: {loss} [ {vol_loss}, {ch_loss}, {sub_loss}]\" )\n","          print(f\"\\n\\n\")\n","\n","        #if (i==200):break\n","        if (i%10==0):print( i, end=\" \" )\n","          \n","    # Epoch Loss & Accuracy\n","    ep_loss = sum(tr_loss)/len(tr_loss);  ep_acc = sum(tr_acc)/len(tr_acc);\n","\n","    ep_vol_loss = sum(tr_vol_loss)/len(tr_vol_loss); ep_vol_acc = sum(tr_vol_acc)/len(tr_vol_acc);  \n","    ep_ch_loss  = sum(tr_ch_loss )/len(tr_ch_loss ); ep_ch_acc  = sum(tr_ch_acc )/len(tr_ch_acc );\n","    ep_sub_loss = sum(tr_sub_loss)/len(tr_sub_loss); ep_sub_acc = sum(tr_sub_acc)/len(tr_sub_acc);\n","\n","    print(\"\\n\")\n","    print(f\"End of Training Epoch {epoch}\")\n","    print(f\"Epoch {epoch} Loss: {ep_loss} [ {ep_vol_loss}, {ep_ch_loss}, {ep_sub_loss}] \")\n","    print(f\"Epoch {epoch} Accr: {ep_acc } [ {ep_vol_acc }, {ep_ch_acc }, {ep_sub_acc }]\\n\")    \n","    \n","    print(f\"Training Loss Epoch: {[ np.round(rloss , 3) for rloss in tr_loss]}\")\n","    print(f\"Training Accuracy Epoch: {tr_acc}\")\n","\n","    return ep_loss, ep_vol_loss, ep_ch_loss, ep_sub_loss, ep_acc, ep_vol_acc, ep_ch_acc, ep_sub_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-6RAcvPtJrD"},"source":["# Similar to train-method define valid funtion is used for testing and validation\n","def valid(ModeL, tst_loader, check_point=150, BATCH_SIZE = VALID_BATCH_SIZE, MODE=\"DEV\" ):\n","\n","    dev_loss=[];     dev_acc=[]  # Loss/Acc-list for each iteration\n","    dev_vol_acc=[];  dev_ch_acc=[];  dev_sub_acc=[]; # Save acc for each level \n","    dev_vol_loss=[]; dev_ch_loss=[]; dev_sub_loss=[];# Save Loss for each level \n","\n","    ModeL.eval()\n","    \n","    n_correct = 0; n_wrong = 0; total = 0\n","    with torch.no_grad():\n","      \n","      for i ,data in enumerate(tst_loader, 0):\n","                \n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","    \n","            # Target values for Volume, Chapter and Subject Levels\n","            target_vol = data['target_vol'].to(device, dtype = torch.long)\n","            target_ch = data['target_ch'].to(device, dtype = torch.long)\n","            target_sub = data['target_sub'].to(device, dtype = torch.long) \n","            if (target_vol.shape[0]!=BATCH_SIZE): break\n","\n","            vol, ch, sub = ModeL(ids, mask) # ModeL prediction\n","            \n","            # Loss estimation (sum losses of each category)\n","            vol_loss = loss_function(vol, target_vol)#.item()\n","            ch_loss  = loss_function(ch , target_ch )#.item()\n","            sub_loss = loss_function(sub, target_sub)#.item()\n","    \n","            loss = vol_loss + ch_loss + sub_loss    \n","\n","\n","            # Calculate accuracy \n","            vol_big_val, vol_big_idx = torch.max(vol.data, dim=1)\n","            ch_big_val, ch_big_idx  = torch.max(ch.data, dim=1)\n","            sub_big_val, sub_big_idx = torch.max(sub.data, dim=1)\n","          \n","            vol_acc, inst1 = calculate_accu(vol_big_idx, target_vol) # Acc. For Volumes\n","            ch_acc,  inst2 = calculate_accu(ch_big_idx, target_ch)   # Acc. For Chapters\n","            sub_acc, inst3 = calculate_accu(sub_big_idx, target_sub) # Acc. For Subjects\n","    \n","            insts= torch.tensor([ inst1.tolist(), inst2.tolist(), inst3.tolist()])\n","    \n","            total_acc = (insts.sum(dim=0)==p1).sum().item()/4  # Acc for [Volumes && Chapaers && Subjects]\n","\n","            # Save Loss and Accuracy for the Current Iteration\n","            dev_vol_loss.append(vol_loss.item()); dev_vol_acc.append(vol_acc);\n","            dev_ch_loss.append(ch_loss.item());   dev_ch_acc.append(ch_acc);\n","            dev_sub_loss.append(sub_loss.item()); dev_sub_acc.append(sub_acc);\n","            dev_loss.append(loss.item());         dev_acc.append(total_acc);\n","\n","            if ( (i % check_point == 0) and (i!=0) ):\n","\n","              print(f\"\\n\\n\\t **************************    {MODE}--Check--point {i}      **************************\")\n","              loss_step = sum(dev_loss[-check_point:]) / len( dev_loss[-check_point:] )\n","              \n","              loss_vol_step = sum(dev_vol_loss[-check_point:]) / len( dev_vol_loss[-check_point:] )\n","              loss_ch_step  = sum(dev_ch_loss[-check_point: ]) / len( dev_ch_loss[-check_point: ] )\n","              loss_sub_step = sum(dev_sub_loss[-check_point:]) / len( dev_sub_loss[-check_point:] )\n","              \n","              \n","              acc_step  = sum(dev_acc[-check_point:]) / len(dev_acc[-check_point:])\n","    \n","              acc_vol_step = sum(dev_vol_acc[-check_point:]) / len(dev_vol_acc[-check_point:])\n","              acc_ch_step  = sum(dev_ch_acc[-check_point: ]) / len(dev_ch_acc[-check_point: ])\n","              acc_sub_step = sum(dev_sub_acc[-check_point:]) / len(dev_sub_acc[-check_point:])\n","    \n","              print(f\"\\t Dev Acc. per {check_point} last steps: {acc_step} [ {acc_vol_step}, {acc_ch_step}, {acc_sub_step}] \")\n","              print(f\"\\t Dev Loss per {check_point} last steps: {loss_step} [ {loss_vol_step}, {loss_ch_step}, {loss_sub_step}]\\n\")\n","              print(f\"\\t Last {15} values\")\n","              print('\\t', dev_loss[-15:])\n","              print('\\t', dev_acc[-15:] )\n","              print()\n","    \n","              # Print --Optionally--\n","              print(\"\\tLast Instance:\")\n","              print(\"\\t\\t\",vol_big_idx, target_vol);  print(\"\\t\\t\",ch_big_idx, target_ch); \n","              print(\"\\t\\t\",sub_big_idx, target_sub);  print(\"\\t\\t\",insts)\n","              print()\n","              print(f\"\\t\\tAcc: {total_acc} [ {vol_acc}, {ch_acc}, {sub_acc}]\")\n","              print(f\"\\t\\tLoss: {loss} [ {vol_loss}, {ch_loss}, {sub_loss}]\" )\n","              print('\\n')\n","\n","            if (i%15==0):print( i, end=\" \" )\n","\n","    # Epoch Loss & Accuracy\n","    print(dev_loss)\n","    print(dev_acc)\n","    total_loss = sum(dev_loss)/len(dev_loss);  \n","    total_acc = sum(dev_acc)/len(dev_acc);\n","\n","    total_vol_loss = sum(dev_vol_loss)/len(dev_vol_loss); total_vol_acc = sum(dev_vol_acc)/len(dev_vol_acc);  \n","    total_ch_loss  = sum(dev_ch_loss )/len(dev_ch_loss ); total_ch_acc  = sum(dev_ch_acc )/len(dev_ch_acc );\n","    total_sub_loss = sum(dev_sub_loss)/len(dev_sub_loss); total_sub_acc = sum(dev_sub_acc)/len(dev_sub_acc);\n","    \n","    print('\\n')\n","    print(f\"End of {MODE} Process:\")\n","    print(f\"Total Loss: {total_loss} [ {total_vol_loss}, {total_ch_loss}, {total_sub_loss}] \")\n","    print(f\"Total Accr: {total_acc } [ {total_vol_acc }, {total_ch_acc }, {total_sub_acc }]\\n\")    \n","\n","    print(f\"{MODE} Loss Epoch: {[ np.round(rloss , 3) for rloss in dev_loss]}\")\n","    print(f\"{MODE} Accuracy Epoch: {dev_acc}\" )\n","\n","    return total_loss, total_vol_loss, total_ch_loss, total_sub_loss, total_acc, total_vol_acc, total_ch_acc, total_sub_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ym88mmBKrcE"},"source":["mFILE = \"models/\"\n","rFILE = \"mResults/\"\n","\n","patience = MAX_PATIENCE\n","\n","today = str(date.today())\n","Name = type(model).__name__ + \"--\" + today + \"-\"\n","\n","# Save values for Training Set -- and --  Validation Set\n","Train_Loss=[]; Train_Acc = [];            Valid_Loss=[]; Valid_Acc = [];\n","Train_Vol_Loss = [];  Train_Vol_Acc = []; Valid_Vol_Loss = [];  Valid_Vol_Acc = [];   \n","Train_Ch_Loss  = [];  Train_Ch_Acc  = []; Valid_Ch_Loss  = [];  Valid_Ch_Acc  = []; \n","Train_Sub_Loss = [];  Train_Sub_Acc = []; Valid_Sub_Loss = [];  Valid_Sub_Acc = []; \n","\n","\n","tr_check_point = int(training_set.len/10) \n","dev_check_point = int(develop_set.len/10)\n","test_check_point = int(testing_set.len/10)\n","\n","print(tr_check_point, dev_check_point, test_check_point)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKSHY_3P2wGO"},"source":["Name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYSZOiMOfc-j"},"source":["EPOCHS = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSuYu_lYZUak"},"source":["p1=torch.ones(TRAIN_BATCH_SIZE)*3 #\n","\n","tr_check_point = 150\n","tr_check_point = 150\n","\n","log_f = open(rFILE + Name + \"-\" + \"system_log.txt\", 'w', encoding='utf-8');  \n","log_f.write(f\"Model: {Name}\\n\\n\")\n","\n","\n","for epoch in range(EPOCHS ):\n","\n","    print(f\" ======================    Epoch: {epoch}    ======================\\n\")\n","    log_f.write(f\" ================    Epoch: {epoch}    ======================\\n\"); \n","\n","    # Training Part\n","    tic = time.time()\n","    print(\"Training:\\n\")\n","    Tr_Loss,Tr_Vol_Loss,Tr_Ch_Loss,Tr_Sub_Loss,Tr_Acc,Tr_Vol_Acc,Tr_Ch_Acc,Tr_Sub_Acc \\\n","                                                  = train(epoch, check_point = tr_check_point)\n","    log_f.write(\"Training:\\n\")\n","    log_f.write(f\"\\tLoss: {Tr_Loss}: [ {Tr_Vol_Loss}, {Tr_Ch_Loss}, {Tr_Sub_Loss} ]\\n\")\n","    log_f.write(f\"\\tAccu: {Tr_Acc }: [ {Tr_Vol_Acc }, {Tr_Ch_Acc }, {Tr_Sub_Acc } ]\\n\\n\")\n","    \n","\n","    # Validation Part\n","    print(\"\\n\\nValidation:\\n\")\n","    Dev_loss, Dev_vol_loss, Dev_ch_loss, Dev_sub_loss, Dev_acc, Dev_vol_acc, Dev_ch_acc, Dev_sub_acc \\\n","                                                  = valid(model, testing_loader, )\n","    log_f.write(f\"Validation:\\n\");\n","    log_f.write(f\"\\tLoss: {Dev_loss} [ {Dev_vol_loss}, {Dev_ch_loss}, {Dev_sub_loss}]\\n\")\n","    log_f.write(f\"\\tAccr: {Dev_acc } [ {Dev_vol_acc }, {Dev_ch_acc }, {Dev_sub_acc }]\\n\")\n","\n","    # Save Epoch's Results\n","    toc = time.time()\n","    print(f\"Epoch Duration: { (toc -tic)/60 } min\\n\")\n","    log_f.write(f\"Epoch Duration: { (toc -tic)/60 } min\\n\")\n","\n","    # Save Results from:  Training  &  Development Set\n","    Train_Loss.append(Tr_Loss); Train_Acc.append(Tr_Acc);\n","    Train_Vol_Loss.append(Tr_Vol_Loss);  Train_Vol_Acc.append(Tr_Vol_Acc);  \n","    Train_Ch_Loss.append( Tr_Ch_Loss)  ; Train_Ch_Acc.append( Tr_Ch_Acc); \n","    Train_Sub_Loss.append(Tr_Sub_Loss);  Train_Sub_Acc.append(Tr_Sub_Acc); \n","\n","    Valid_Loss.append(Dev_loss); Valid_Acc.append(Dev_acc);\n","    Valid_Vol_Loss.append(Dev_vol_loss);  Valid_Vol_Acc.append(Dev_vol_acc);  \n","    Valid_Ch_Loss.append( Dev_ch_loss)  ; Valid_Ch_Acc.append( Dev_ch_acc); \n","    Valid_Sub_Loss.append(Dev_sub_loss);  Valid_Sub_Acc.append(Dev_sub_acc);\n","\n","\n","    # Save Best Model Based on Total Loss\n","    if (Valid_Loss[-1] <= min(Valid_Loss)):\n","\n","      patience = MAX_PATIENCE;\n","      mName = mFILE + Name+\"-model-\"+str(epoch)+\".pt\"\n","      torch.save(model.state_dict(), mName)\n","      log_f.write(f\"Save Model: {mName} on Epoch {epoch} \\n\\n\")\n","      print()\n","      print( f\"Saved Model: {mName} of Epoch: {epoch}.\\n\")\n","      print(f\"Patience: {patience}\")\n","\n","    else:\n","      patience = patience - 1\n","    if (patience == 0):break\n","    print('\\n\\n')\n","\n","\n","log_f.close()\n","print(\"End of Training Process\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16hnpglzOabx"},"source":["%matplotlib inline\n","from matplotlib import pyplot as plt\n","\n","x_axis = [i+1 for i in range(EPOCHS)]\n","\n","fig = plt.figure(figsize=(12, 8))\n","ax = fig.add_subplot(211)\n","ax.plot( x_axis, Train_Acc, marker=\"*\", color=\"black\", markersize=12)\n","ax.plot( x_axis, Valid_Acc, marker=\"*\", color=\"black\", markersize=12)\n","ax.plot( x_axis, Train_Acc, label='Train Accuracy' , color='red', linewidth=2.5) ;\n","ax.plot( x_axis, Valid_Acc, label='Valid Accuracy', color='purple', linewidth=2.5)\n","plt.title(f\"{Name}--Accuracy\"); ax.legend(loc=\"lower right\")\n","\n","ax = fig.add_subplot(212)\n","ax.plot( x_axis, Train_Loss, marker=\"*\", color=\"navy\", markersize=12)\n","ax.plot( x_axis, Valid_Loss, marker=\"*\", color=\"navy\", markersize=12)\n","ax.plot( x_axis, Train_Loss, label='Train Loss', color='red', linewidth=2.5)\n","ax.plot( x_axis, Valid_Loss, label='Valid Loss', color='purple', linewidth=2.5)\n","plt.title(f\"{Name}--Loss\");  ax.legend()\n","\n","plt.savefig(\"mResults/\"+Name+\"-Acc\\Loss-Plot.png\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsvxqSvSABB5"},"source":["---\n","# **`TESTING SECTION`**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"GfCz_2bg8KJJ"},"source":["from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHSUkuHfmn5y"},"source":["FILE = \"mResults/\" # log-file Position\n","#tName = \"models/Last3_BERT--2021-05-03--model-2.pt\" # Saving Position\n","tName = \"models/SumLast_BERT--2021-05-05--model-2.pt\" # Saving Position\n","tName = \"models/SumLast4_BERT--2021-05-06--model-2.pt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1c6KGJufiwIz"},"source":["#Model = SimpleBERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub,)\n","#Model = Last3_BERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub,)\n","#Model = Mask_SimpleBERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub, Chapter_matrix= Ch_matrix, Subject_matrix=Sub_matrix)\n","Model = SumLast4_BERT(bert_model=BERT_MODEL, vol_num=Unique_Vol , ch_num= Unique_Ch, sub_num= Unique_Sub)\n","\n","Model.to(device)\n","Model.load_state_dict(torch.load( tName ))\n","Model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekCXvUZ3uTWF"},"source":["log_f = open( FILE + Name +\"-\"+'system_log.txt', 'a', encoding='utf-8'); \n","log_f.write(f\"\\n\\n\\n\"); log_f.flush()\n","\n","\n","Test_loss, Test_vol_loss, Test_ch_loss, Test_sub_loss, Test_acc, Test_vol_acc, Test_ch_acc, Test_sub_acc \\\n","                                        = valid(model, testing_loader, test_check_point, TEST_BATCH_SIZE, \"Test\")\n","\n","# Write Results on log_f File\n","\n","log_f.write(f\" ================    TESTING    ======================\\n\");\n","log_f.write(f\"Loss: {Test_loss} [ {Test_vol_loss}, {Test_ch_loss}, {Test_sub_loss}]\\n\")\n","log_f.write(f\"Accr: {Test_acc } [ {Test_vol_acc }, {Test_ch_acc }, {Test_sub_acc }]\\n\\n\\n\")\n","log_f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxsQjF20xe94"},"source":["---\n","**Accuracy--Macro_Avg--Weighted_Avg**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"AbrheOMkztZf"},"source":["Model = model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIK6N2M4bbG5"},"source":["true_vol_output = []; pred_vol_output = []\n","true_ch_output  = []; pred_ch_output  = []\n","true_sub_output = []; pred_sub_output = []\n","\n","for i ,data in enumerate(develop_loader, 0):\n","  ids = data['ids'].to(device, dtype = torch.long)\n","  mask = data['mask'].to(device, dtype = torch.long)\n","\n","  # Target values for Volume, Chapter and Subject Levels\n","  target_vol = data['target_vol'].to(device, dtype = torch.long)\n","  target_ch = data['target_ch'].to(device, dtype = torch.long)\n","  target_sub = data['target_sub'].to(device, dtype = torch.long)\n","\n","  if (target_vol.shape[0]!=TEST_BATCH_SIZE): break\n","\n","  true_vol_output.extend(target_vol.tolist())\n","  true_ch_output.extend (target_ch.tolist() )\n","  true_sub_output.extend(target_sub.tolist())\n","\n","  vol, ch, sub = Model(ids, mask)  \n","  #print(vol)\n","\n","  vol_big_val, vol_big_idx = torch.max(vol.data, dim=1)\n","  #print(vol_big_idx.tolist(), target_vol)\n","\n","  ch_big_val, ch_big_idx  = torch.max(ch.data, dim=1)\n","  #print(ch_big_idx.tolist(), target_ch)\n","\n","  sub_big_val, sub_big_idx = torch.max(sub.data, dim=1)\n","  #print(sub_big_idx.tolist(), target_sub )\n","\n","  pred_vol_output.extend(vol_big_idx.tolist())\n","  pred_ch_output.extend (ch_big_idx.tolist() )\n","  pred_sub_output.extend(sub_big_idx.tolist())\n","\n","  vol_acc, inst1 = calculate_accu(vol_big_idx, target_vol) \n","  ch_acc,  inst2 = calculate_accu(ch_big_idx, target_ch)   \n","  sub_acc, inst3 = calculate_accu(sub_big_idx, target_sub) \n","\n","  insts= torch.tensor([ inst1.tolist(), inst2.tolist(), inst3.tolist()])\n","  #print(insts)\n","\n","  p1=torch.ones(4)*3\n","  total_acc = (insts.sum(dim=0)==p1).sum().item()/4\n","  #print(f\"Acc: {total_acc} [ {vol_acc}, {ch_acc}, {sub_acc}]\" )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DydmzboQe726"},"source":["print(len(true_ch_output), len(pred_ch_output),\"\\n\")\n","\n","print(true_vol_output[:50])\n","print(pred_vol_output[:50])\n","print(len(list(np.unique(np.array(pred_vol_output)))), list(np.unique(np.array(pred_vol_output))))\n","print(len(list(np.unique(np.array(true_vol_output)))), list(np.unique(np.array(true_vol_output))))\n","\n","print() \n","print( true_ch_output [:50])\n","print( pred_ch_output [:50])\n","print(len(list(np.unique(np.array(pred_ch_output)))), list(np.unique(np.array(pred_ch_output))))\n","print(len(list(np.unique(np.array(true_ch_output)))), list(np.unique(np.array(true_ch_output))))\n","\n","print() \n","print(true_sub_output[:50])\n","print(pred_sub_output[:50])\n","print(len(list(np.unique(np.array(pred_sub_output)))), list(np.unique(np.array(pred_sub_output))))\n","print(len(list(np.unique(np.array(true_sub_output)))), list(np.unique(np.array(true_sub_output))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AF8r-9okp3qp"},"source":["log_f = open( FILE + Name +\"-\"+'system_log.txt', 'a', encoding='utf-8'); \n","\n","# VOLUMES:  Accuracy-Macro_avg-Weighted-Avg\n","vol_cr_output = classification_report(  y_true=true_vol_output,  y_pred=pred_vol_output, output_dict=True,\n","                              target_names=[get_key(vol_label_dict, i)[0] for i in range(len(vol_label_dict))],\n","                              labels=[i for i in range(len(vol_label_dict))], )\n","\n","log_f.write(\"VOLUMES:  Accuracy---Macro_avg---Weighted-Avg\\n\")\n","log_f.write(str(vol_cr_output[\"accuracy\"])    ); log_f.write(\"\\n\")\n","log_f.write(str(vol_cr_output[\"macro avg\"])   ); log_f.write(\"\\n\")\n","log_f.write(str(vol_cr_output[\"weighted avg\"])); log_f.write(\"\\n\\n\\n\")\n","\n","\n","# CHAPTERS:  Accuracy-Macro_avg-Weighted-Avg\n","ch_cr_output = classification_report(  y_true=true_ch_output,  y_pred=pred_ch_output, output_dict=True,)\n","                              #target_names=[get_key(ch_label_dict, i)[0] for i in range(len(ch_label_dict))],\n","                              #labels=[i for i in range(len(ch_label_dict))], )\n","\n","log_f.write(\"CHAPTERS:  Accuracy---Macro_avg---Weighted-Avg\\n\")\n","log_f.write(str(ch_cr_output[\"accuracy\"])    ); log_f.write(\"\\n\")\n","log_f.write(str(ch_cr_output[\"macro avg\"])   ); log_f.write(\"\\n\")\n","log_f.write(str(ch_cr_output[\"weighted avg\"])); log_f.write(\"\\n\\n\\n\")\n","\n","\n","# SUBJECTS:  Accuracy-Macro_avg-Weighted-Avg\n","sub_cr_output = classification_report(  y_true=true_sub_output,  y_pred=pred_sub_output, output_dict=True,\n","                              target_names=[get_key(sub_label_dict, i)[0] for i in range(len(sub_label_dict))],\n","                              labels=[i for i in range(len(sub_label_dict))], )\n","\n","log_f.write(\"SUBJECTS:  Accuracy---Macro_avg---Weighted-Avg\\n\")\n","log_f.write(str(sub_cr_output[\"accuracy\"])    ); log_f.write(\"\\n\")\n","log_f.write(str(sub_cr_output[\"macro avg\"])   ); log_f.write(\"\\n\")\n","log_f.write(str(sub_cr_output[\"weighted avg\"])); log_f.write(\"\\n\\n\\n\")\n","\n","log_f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaMVPbY0e8iD"},"source":["print(classification_report(  y_true=true_vol_output,  y_pred=pred_vol_output,\n","                              target_names=[get_key(vol_label_dict, i)[0] for i in range(len(vol_label_dict))],\n","                              labels=[i for i in range(len(vol_label_dict))],\n","                                #labels=[e for e in inv_class_dict.keys() if e != 15],\n","                                #target_names=[e for e in class_dict.keys() if e != 'X'],\n","                                output_dict=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1wDQsG0wDhr"},"source":["print(classification_report(  y_true=true_ch_output,  y_pred=pred_ch_output,\n","                              target_names=[get_key(ch_label_dict, i)[0] for i in range(len(ch_label_dict))],\n","                              labels=[i for i in range(len(ch_label_dict))],\n","                                #labels=[e for e in inv_class_dict.keys() if e != 15],\n","                                #target_names=[e for e in class_dict.keys() if e != 'X'],\n","                                output_dict=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGePnhJ3v2pS"},"source":["print(classification_report(  y_true=true_sub_output,  y_pred=pred_sub_output,\n","                              target_names=[get_key(sub_label_dict, i)[0] for i in range(len(sub_label_dict))],\n","                              labels=[i for i in range(len(sub_label_dict))],\n","                                #labels=[e for e in inv_class_dict.keys() if e != 15],\n","                                #target_names=[e for e in class_dict.keys() if e != 'X'],\n","                                output_dict=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uC6hE8ipe8lu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKp219nTe8rz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyIid4Vj14ez"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZ_6oYaY14rS"},"source":[""],"execution_count":null,"outputs":[]}]}